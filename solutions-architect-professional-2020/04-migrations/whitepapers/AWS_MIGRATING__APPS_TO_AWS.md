# Migrating Applications Running Relational Databases to AWS

**Best Practices Guide**

Terms

homogeneous migration - data base applications migrated between two databases of the same engine types

heterogeneous migration - data base applications migrated between two databases of different engine types

## Migration Steps and Tools

1. Migration assessment analysis
2. Schema conversion to a target database platform
3. SQL statement and application code conversion
4. Data migration
5. Testing of converted database and application
6. Setting up replication and failover scenarios for data migration to the target platform
6. Setting up replication and failover scenarios for data migration to the target plaform
7. Setting up monitoring for a new production environment and go live with the target environment

Tools

- AWS Schema Conversion Tool (AWS SCT) - automates the conversion of data base object from different database migration systems to different RDS database targets
- AWS Database Migration Service (AWS DMS) - A service for data migration to and from AWS database targets. This can be used for replication tasks.

### Development Environment Setup Prerequisites

Setup a development environment that mirrors the production environment. Download and install AWS SCT on the server in the development environment and use an RDS database as the migration target. 

### 1. Migration Assessment

- Identify schema objects
- Recommend the best target engine
- Recommend other AWS services that can substitute missing features
- Recommends unique features in RDS that can save licensing costs
- Recommends re-architecting for the cloud e.g. sharding into multiple instances by customer, geography, or partition key

Take advantage of the report generated by AWS SCT. This will give you a better understanding of what the conversion will take.

### 2. Schema Conversion

This is a two step process of:

1. Converting the schema
2. Then applying it to the target database

The conversion with AWS SCT includes triggers, stored procedures, and functions. This will account for 30% of the migration. The tool also allows for custom schema transformations. Once the rules have been created then it's time to use the AWS DMS tool to import them.

### 3. Conversion of Embedded SQL and Application Code

Next update the custom scripts with embedded SQL statements (ETL scripts, reports, etc) and the code that has been accessing the database. AWS SCT can scan code and extract the embedded statements and covert as many as it can automatically and flag the rest. This is roughly 15% of the effort of a migration.

Conversion of application code should look something like the following:

1. Run an assessment report to understand the level of effort required for the conversion
2. Analyze the code to extract the embedded SQL statements.
3. Allow the AWS SCT to automatically convert as much code as possible.
4. Work through the remaining conversion Action Items manually
5. Save code changes

When performing the conversion attach all effected projects within the AWS SCT tool.

### 4. Data Migration

Now migrate data from the source database to the target database. This can be done using AWS DMS. Once the migration is complete perform testing on the new schema and application.

AWS DMS works by creating a replication server that acts as a middleman between the target and source databases. There are three main steps to getting the data migration service up and running:

1. Set up a replication instance
2. Define connections for the source and target databases.
3. Define data replication tasks


When performing the migration there are two ways to migrate the data:

1. A full migration of data
2. A full migration of data followed with continuous replication

AWS DMS will not create secondary indexes, non-primary key constraints, or data defaults or other database objects like stored procedure, views, functions, packages, etc. This is where the AWS SCT tool comes in handle because of the conversion that where run earlier. The `mapping rules` can be used by AWS DMS to make the changes.

### 5. Testing Converted Code

In the testing phase aim for roughly the same about of time that was spent in the development phase **45%**. 

The goal of testing is to:

1. Exercise critical functionality in the application
2. Verifying that converted SQL object are functioning as intended.

Test that data rows are affected the same, there are no lurking data issues, anything that could cause issues with the user experience or anything of that nature.

### 6. Data Replication

There are applications that cannot handle a downtime long enough to tolerate a downtime window long enough to migrate all the data in a full load. For this we use the Change Data Capture Process to implement on going replication from the data source to the target database. AWS DMS will manage this on going process with minimal load on the source database.

Look up specific rules necessary for the conversion of your specific database.

### 7. Deployment to AWS and Go Live

Test the data migration of the production database to ensure that all data can be successfully migrated. You can **Enable Validation** in the task settings of DMS to validate that the data migration is working by comparing the data in the source and target databases.

**Design simple rollback plans in the unlikely event that unrecoverable errors occur during the go live window.** Due to AWS SCT and AWS DMS working together to preserve the original source database and application this can be as simple as scripts to point the connection strings back to the original source database.

### Post Deployment Monitoring

AWS DMS monitors the number of rows inserted, deleted, and updated, as well as the number of DDL statements issued per table while a task is running. These can be viewed on the Table Statistics under the Database migration task, there will also be metrics under the Migration Task Metrics pane.


## Best Practices

### Schema Conversion

- Save the database migration assessment report that was generated, this can be a valuable project management tool.
- For most conversion, apply DDL to target database in the following order: 
    - Sequences
    - Tables
    - Views
    - Procedures
    - Functions should be applied to the target database in order of dependency e.g. if a function is referenced in a table column then that function must be created first.
- Configure the AWS SCT with the memory performance settings you need - increasing the memory can speed up performance, but slow down the desktop that you are running on.
- Apply the additional schema that AWS SCT creates to the target db: this is usually to polyfill features missing between database platforms.
- Use source code version control to track changes to target objects 

### Application Code Conversion

- After running the initial application assessment report save it as a CSV and PDF

### Data Migration

- Choose a replication instance class large enough to support your database size and transactional load
- On target database, disable what is not needed - jobs, foreign keys, triggers, validation, and secondary indexes.
- Tables in the source database that do not participate in common transactions can be allocated to different tasks
- Monitor performance of the source system to ensure it is able to handle the load of the database migration tasks
- Enable logging using CloudWatch logs
- Optimize the loading of Blobs (if your source has them) in the task settings

### Data Replication

- Achieve best performance by not applying indexes or foreign keys to the target database during the initial load, once the initial load is complete then these changes can be applied.
- Apply indexes and foreign keys to the target database before the application is ready to go live.
- For on going replication enable Multi-AZ option on the replication instance
- Use the AWS API/CLI for more advanced DMS settings not available from the console
- Disable backups on the target database during the full load for better performance, then enable during the cutover.
- Wait until cutover to make your target RDS instance Multi-AZ for better performance

### Testing

- Have a test environment where full regression tests of the original application can be conducted
- In the absence of automated testing run smoke tests on old and new applications
- Apply standard practices for database-driven software testing regardless of the migration process
- Have sample test data that is used only for testing
- Know your data logic and apply it to your test plans
- Test using a dataset similar in size to the production dataset to expose performance bottlenecks

### Deployment and Go Live

- Have a rollback plan in place
- Test the deployment on a staging or pre-production environment 
- Verify that AWS DMS has reached a steady state and all existing data has been replicated to the new server before cutting off access to the old application
- Verify that database maintenance jobs are in place, backups and index maintenance
- Turn on Multi-AZ, if required
- Verify monitoring is in place
- Take advantage of services that make deployment easier: CloudFormation, OpsWorks, CodeDeploy

### Post-Deployment Monitoring

- Create CloudWatch Logs alarms for unusual activity
- Monitor logs and exception reports
- Determine if there are additional platform specific metrics to capture
- Monitor instance health


## Challenges

- Use the AWS SCT Tools to convert between two databases
- Use AWS DMS to migrate a database
- Use the AWS SCT + AWS DMS tool to populate a test database with production data, hiding real consumer data like SSNs etc.

## Resources

- https://aws.amazon.com/blogs/database/aws-schema-conversion-tool-blog-series-introducing-new-features-in-build-617/